{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIQUsG9riSgz1rpFLhBZXt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Test input auto segmentasi multi aspek\n"],"metadata":{"id":"eizmYdRWIpKF"}},{"cell_type":"code","source":["# ==== IMPORTS ====\n","import os, re, json, pickle\n","import numpy as np\n","import pandas as pd\n","\n","from gensim.corpora import Dictionary\n","from gensim.models.ldamodel import LdaModel\n","\n","# ==== PATHS ====\n","ROOT        = \"/content/drive/MyDrive/SKRIPSI/COBA_LDA/MODEL_LDA_5ASPEK_NEW\"\n","MODEL_DIR   = os.path.join(ROOT, \"Model_LDA\")\n","ARTEFAK_DIR = os.path.join(ROOT, \"artefak\")\n","\n","# ==== LOAD MODEL & ARTEFAK LDA ====\n","dictionary = Dictionary.load(os.path.join(MODEL_DIR, \"dictionary.gensim\"))\n","lda        = LdaModel.load(os.path.join(MODEL_DIR, \"lda_model.gensim\"))\n","\n","bigram = None\n","bg_path = os.path.join(MODEL_DIR, \"bigram_phraser.pkl\")\n","if os.path.exists(bg_path):\n","    with open(bg_path, \"rb\") as f:\n","        bigram = pickle.load(f)\n","    print(\"✅ Bigram phraser dimuat.\")\n","else:\n","    print(\"⚠️ Bigram phraser TIDAK ditemukan, jalan tanpa bigram.\")\n","\n","# mapping topik → aspek\n","map_xlsx = os.path.join(MODEL_DIR, \"mapping_aspek_auto.xlsx\")\n","df_map = pd.read_excel(map_xlsx, engine=\"openpyxl\")\n","topic2aspect = dict(zip(df_map[\"topic\"], df_map[\"assigned_aspect\"]))\n","\n","# daftar aspek\n","ASPEK = [\"Kemasan\",\"Aroma\",\"Tekstur\",\"Harga\",\"Efek\"]\n","\n","# seed words\n","seeds_json = os.path.join(ARTEFAK_DIR, \"seeds.json\")\n","with open(seeds_json, \"r\", encoding=\"utf-8\") as f:\n","    sj = json.load(f)\n","\n","SEED_DICT = {\n","    \"Kemasan\": set(sj.get(\"Kemasan\", [])),\n","    \"Aroma\":   set(sj.get(\"Aroma\", [])),\n","    \"Tekstur\": set(sj.get(\"Tekstur\", [])),\n","    \"Harga\":   set(sj.get(\"Harga\", [])),\n","    \"Efek\":    set(sj.get(\"Efek\", [])),\n","}\n","\n","SEED_ROOTS = {\n","    aspek: set(\n","        _root_id(part)\n","        for w in SEED_DICT[aspek]\n","        for part in (w.split('_') + [w])\n","    )\n","    for aspek in ASPEK\n","}\n","\n","print(\"✅ Model, dictionary, mapping, dan seeds dimuat.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaRiCvv3QtVQ","executionInfo":{"status":"ok","timestamp":1764172969561,"user_tz":-420,"elapsed":121,"user":{"displayName":"21-093_Adz Dzikry Pradana Putra","userId":"01565016450624471857"}},"outputId":"c267938d-43a8-4b46-8305-b3fc03cd8267"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Bigram phraser dimuat.\n","✅ Model, dictionary, mapping, dan seeds dimuat.\n"]}]},{"cell_type":"code","source":["# ==== UTIL PREPROC ====\n","\n","def _simple_clean(text: str) -> str:\n","    t = str(text).lower()\n","    t = t.replace(\"enggak\",\"gak\").replace(\"nggak\",\"gak\")\n","    return re.sub(r\"[^a-z0-9_ ]+\", \" \", t)\n","\n","def tokenize_from_val(val):\n","    \"\"\"Terima string atau list token → list token (+bigram jika ada)\"\"\"\n","    if isinstance(val, list):\n","        toks = [str(t) for t in val if t]\n","    else:\n","        toks = _simple_clean(val).split()\n","    if bigram is not None:\n","        try:\n","            toks = list(bigram[toks])\n","        except Exception:\n","            pass\n","    return toks\n","\n","def bow_of(tokens):\n","    return dictionary.doc2bow([t for t in tokens if t in dictionary.token2id])\n","\n","def _root_id(token: str) -> str:\n","    t = str(token).lower().strip()      # <<< PERBAIKAN PENTING\n","    t = re.sub(r'(ku|mu|nya)$', '', t)  # hilangkan akhiran umum\n","    t = re.sub(r'^([a-z0-9]+)_\\1$', r'\\1', t)  # merah_merah -> merah\n","    return t\n","\n","def _expand_for_seed(tokens):\n","    \"\"\"Split underscores + akar ringan agar seed match robust.\"\"\"\n","    parts = []\n","    for tok in tokens:\n","        if '_' in tok:\n","            parts.extend(tok.split('_'))\n","        parts.append(tok)\n","    return {_root_id(p) for p in parts}"],"metadata":{"id":"HM3K6nIjQnh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# akar kata simpel untuk masing-masing aspek\n","BASE_ROOT = {\n","    \"Kemasan\": \"kemasan\",\n","    \"Aroma\":   \"aroma\",\n","    \"Tekstur\": \"tekstur\",\n","    \"Harga\":   \"harga\",\n","    \"Efek\":    \"efek\",\n","}\n","\n","def build_aspect_anchors_from_seeds():\n","    \"\"\"\n","    Bangun ASPECT_ANCHORS dari SEED_DICT:\n","    - Ambil semua seed per aspek.\n","    - Pecah underscore, ambil root-nya.\n","    - Kalau root mengandung BASE_ROOT[aspek], jadikan anchor.\n","    Hasilnya: subset kecil seed yang kata-katanya eksplisit mengandung label aspek.\n","    \"\"\"\n","    anchors = {a: set() for a in ASPEK}\n","\n","    for aspek in ASPEK:\n","        base = BASE_ROOT[aspek]\n","        for w in SEED_DICT[aspek]:\n","            # pecah seed kalau pakai underscore, misal 'kemasannya_lucu'\n","            parts = w.split('_')\n","            for part in parts:\n","                r = _root_id(part)\n","                if base in r:\n","                    anchors[aspek].add(r)\n","\n","    return anchors\n","\n","ASPECT_ANCHORS = build_aspect_anchors_from_seeds()\n","print(\"ASPECT_ANCHORS:\", ASPECT_ANCHORS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ktfstwE3mgqI","executionInfo":{"status":"ok","timestamp":1764172969593,"user_tz":-420,"elapsed":25,"user":{"displayName":"21-093_Adz Dzikry Pradana Putra","userId":"01565016450624471857"}},"outputId":"89bf99f2-999a-4ac8-e3f9-d1613cb5ecaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ASPECT_ANCHORS: {'Kemasan': {'kemasan'}, 'Aroma': {'aroma', 'aromatik'}, 'Tekstur': {'teksture', 'tekstur', 'bertekstur', 'teksturny'}, 'Harga': {'harga'}, 'Efek': {'ngefek', 'efektif', 'efek'}}\n"]}]},{"cell_type":"code","source":["# ==== BOOSTING & PREDIKSI ====\n","\n","def predict_aspect_boosted(\n","    tokens,\n","    lambda_boost=0.9,   # seberapa kuat pengaruh seed\n","    gamma=2.0,          # non-linear gain\n","    seed_bonus=0.03,    # bonus additive kecil per seed\n","    dampen_price_if_no_seed=True,\n","    price_delta=0.7,    # turunkan Harga bila tak ada seed & ada seed aspek lain\n","    prefer_seed_for_top1=True\n","):\n","    # 1) Distribusi LDA per topik → [(k,p)]\n","    bow = bow_of(tokens)\n","    dist_pairs = lda.get_document_topics(bow, minimum_probability=0.0)\n","\n","    # 2) Agregasi topik → aspek (RAW)\n","    p_aspek = {a: 0.0 for a in ASPEK}\n","    for k, p in dist_pairs:\n","        a = topic2aspect.get(k, f\"T{k}\")\n","        p_aspek[a] += p\n","\n","    # 3) Seed hits\n","    toks_for_seed = _expand_for_seed(tokens) | _expand_for_seed(_simple_clean(\" \".join(tokens)).split())\n","    seed_hits = {\n","        a: len({_root_id(w) for w in SEED_DICT[a]} & toks_for_seed)\n","        for a in ASPEK\n","    }\n","\n","    # 4) Boost semua aspek berdasar seed (non-linear) + bonus kecil\n","    p_boost = {\n","        a: p_aspek[a] * (1.0 + lambda_boost * seed_hits[a])**gamma\n","        for a in ASPEK\n","    }\n","    for a, h in seed_hits.items():\n","        if h >= 1:\n","            p_boost[a] += seed_bonus * h\n","\n","    # 5) Gating khusus Harga\n","    if dampen_price_if_no_seed and seed_hits[\"Harga\"] == 0 and max(seed_hits.values()) > 0:\n","        p_boost[\"Harga\"] *= price_delta\n","\n","    # 6) Normalisasi\n","    Z = sum(p_boost.values()) or 1.0\n","    p_boost = {a: v / Z for a, v in p_boost.items()}\n","\n","    # 7) Label final (prioritaskan aspek yang kena seed jika ada)\n","    if prefer_seed_for_top1 and any(h > 0 for h in seed_hits.values()):\n","        seeded_aspects = [a for a,h in seed_hits.items() if h > 0]\n","        aspect_final = max(seeded_aspects, key=lambda a: p_boost[a])\n","    else:\n","        aspect_final = max(p_boost, key=p_boost.get)\n","\n","    # (opsional) juga kembalikan top-1 murni tanpa preferensi seed\n","    aspect_top1_plain = max(p_boost, key=p_boost.get)\n","\n","    return p_aspek, seed_hits, p_boost, aspect_final, aspect_top1_plain\n","\n","def select_multi_smart(p_boost, seed_hits, thr=0.35, max_k=2):\n","    \"\"\"Pilih multi-aspek: >=thr atau ada seed; maksimal max_k item.\"\"\"\n","    items = sorted(p_boost.items(), key=lambda x: x[1], reverse=True)\n","    must  = [a for a,h in seed_hits.items() if h >= 1]\n","    cand  = [a for a,p in items if (p >= thr) or (a in must)]\n","    picked= [(a, p_boost[a]) for a,_ in items if a in cand][:max_k]\n","    return [(a, float(f\"{p:.4f}\")) for a,p in picked]"],"metadata":{"id":"zyZUeK_-Qg9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# kata-kata yang TIDAK boleh memicu split aspek\n","SEGMENT_STOPWORDS = {\n","    \"tidak\", \"gak\", \"nggak\", \"enggak\", \"ga\",\n","    \"banget\", \"aja\", \"sih\", \"dong\", \"kok\",\n","    \"dan\", \"atau\", \"yang\", \"itu\", \"ini\",\n","    # tambahkan kata-kata umum yang sering muncul lintas aspek:\n","    \"enak\", \"dipake\", \"pake\", \"nyaman\"\n","}\n","\n","def detect_aspect_from_token(tok: str):\n","    \"\"\"\n","    DETEKSI ASPEK khusus SEGMENTASI:\n","    - pakai SEED_ROOTS\n","    - ABAIKAN kata-kata umum/negasi di SEGMENT_STOPWORDS\n","    \"\"\"\n","    root = _root_id(_simple_clean(tok)).strip()\n","    if not root or root in SEGMENT_STOPWORDS:\n","        return None\n","\n","    for a in ASPEK:\n","        if root in SEED_ROOTS[a]:\n","            return a\n","\n","    return None"],"metadata":{"id":"Kx7oANh-qz_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# akar kata sederhana untuk tiap aspek\n","BASE_ROOT = {\n","    \"Kemasan\": \"kemas\",\n","    \"Aroma\":   \"aroma\",\n","    \"Tekstur\": \"tekstur\",\n","    \"Harga\":   \"harga\",\n","    \"Efek\":    \"efek\",\n","}\n","\n","def split_into_sentences(text: str):\n","    \"\"\"\n","    Pecah teks jadi kalimat berdasarkan . ! ?\n","    \"\"\"\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    parts = re.split(r'([.!?])', text)\n","    sents = []\n","    buf = \"\"\n","\n","    for part in parts:\n","        if part in [\".\", \"!\", \"?\"]:\n","            buf += part\n","            if buf.strip():\n","                sents.append(buf.strip())\n","            buf = \"\"\n","        else:\n","            buf += part\n","\n","    if buf.strip():\n","        sents.append(buf.strip())\n","\n","    return sents\n","\n","\n","def segment_text_for_aspect(text):\n","    \"\"\"\n","    Segmentasi berbasis ANCHOR aspek:\n","    - Cari token yang mengandung akar 'kemas', 'aroma', 'tekstur', 'harga', 'efek'.\n","    - Tiap anchor dianggap awal segmen baru untuk aspek itu.\n","    - Segmen = rentang dari anchor_i sampai sebelum anchor_{i+1}.\n","    - Bagian sebelum anchor pertama ikut segmen pertama.\n","    \"\"\"\n","    sentences = split_into_sentences(text)\n","    segments = []\n","\n","    for sent in sentences:\n","        tokens = sent.split()\n","        if not tokens:\n","            continue\n","\n","        # cari anchor: posisi token yang mengandung base-root aspek\n","        anchor_list = []   # list of (pos, aspek)\n","        for idx, tok in enumerate(tokens):\n","            root = _root_id(_simple_clean(tok))\n","            for aspek in ASPEK:\n","                base = BASE_ROOT[aspek]\n","                if base in root:\n","                    anchor_list.append((idx, aspek))\n","                    break\n","\n","        if not anchor_list:\n","            # tidak ada anchor aspek di kalimat ini → 1 segmen utuh\n","            segments.append(sent.strip())\n","            continue\n","\n","        # kompres anchor: buang anchor berurutan dengan aspek yang sama\n","        compressed = []\n","        for pos, asp in sorted(anchor_list, key=lambda x: x[0]):\n","            if not compressed or compressed[-1][1] != asp:\n","                compressed.append((pos, asp))\n","\n","        # buat boundaries: dari anchor ke anchor berikutnya\n","        for i, (pos, asp) in enumerate(compressed):\n","            start = pos if i > 0 else 0\n","            end = compressed[i+1][0] if i+1 < len(compressed) else len(tokens)\n","\n","            seg_tokens = tokens[start:end]\n","            seg_text = \" \".join(seg_tokens).strip(\" ,\")\n","            if seg_text:\n","                segments.append(seg_text)\n","\n","    return segments"],"metadata":{"id":"O_si7tefQZTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_segmented_text(text,\n","                        lambda_boost=0.9,\n","                        gamma=2.0,\n","                        seed_bonus=0.03,\n","                        dampen_price_if_no_seed=True,\n","                        price_delta=0.7,\n","                        prefer_seed_for_top1=True,\n","                        max_head_tokens=4):\n","    \"\"\"\n","    1) Segmentasi awal: segment_text_for_aspect(text)\n","    2) Klasifikasi aspek untuk tiap segmen\n","    3) Perbaiki kepala segmen:\n","       - kalau beberapa token awal segmen i masih nyambung dgn aspect_final segmen i-1,\n","         (asp token = None atau sama dgn aspek sebelumnya, dan minimal 1 token = aspek sebelumnya)\n","         → head dipindah ke akhir segmen i-1.\n","    4) Merge segmen berurutan yg aspek-nya sama.\n","    \"\"\"\n","\n","    # ===== 0) Segmentasi awal =====\n","    segs_raw = segment_text_for_aspect(text)\n","\n","    # ===== 1) Klasifikasi awal =====\n","    raw = []\n","    for seg in segs_raw:\n","        toks = tokenize_from_val(seg)\n","\n","        p_raw, hits, p_boost, aspect_final, aspect_top1_plain = predict_aspect_boosted(\n","            toks,\n","            lambda_boost=lambda_boost,\n","            gamma=gamma,\n","            seed_bonus=seed_bonus,\n","            dampen_price_if_no_seed=dampen_price_if_no_seed,\n","            price_delta=price_delta,\n","            prefer_seed_for_top1=prefer_seed_for_top1\n","        )\n","        prob_final = p_boost[aspect_final]\n","\n","        raw.append({\n","            \"seg_text\": seg,\n","            \"tokens\": toks,\n","            \"p_boost\": p_boost,\n","            \"seed_hits\": hits,\n","            \"aspect_final\": aspect_final,\n","            \"aspect_prob_final\": prob_final,\n","        })\n","\n","    # ===== 2) Perbaiki kepala segmen (head tokens) =====\n","    adjusted = []\n","    for idx, item in enumerate(raw):\n","        if idx == 0:\n","            adjusted.append(item)\n","            continue\n","\n","        prev = adjusted[-1]\n","        prev_aspect = prev[\"aspect_final\"]\n","\n","        toks = item[\"tokens\"]\n","        if len(toks) == 0:\n","            continue\n","\n","        head_len = 0\n","        seen_prev_aspect = False\n","\n","        for t in toks:\n","            asp_tok = detect_aspect_from_token(t)\n","            if asp_tok is None or asp_tok == prev_aspect:\n","                head_len += 1\n","                if asp_tok == prev_aspect:\n","                    seen_prev_aspect = True\n","            else:\n","                break\n","\n","            if head_len >= max_head_tokens:\n","                break\n","\n","        if not seen_prev_aspect or head_len == 0 or head_len >= len(toks):\n","            adjusted.append(item)\n","            continue\n","\n","        # geser head ke segmen sebelumnya\n","        head_tokens = toks[:head_len]\n","        tail_tokens = toks[head_len:]\n","\n","        moved_text = \" \".join(head_tokens)\n","        tail_text  = \" \".join(tail_tokens).strip()\n","\n","        # update segmen sebelumnya\n","        new_prev_text = prev[\"seg_text\"].rstrip(\" ,\") + \" \" + moved_text\n","        new_prev_tokens = tokenize_from_val(new_prev_text)\n","        p_raw_p, hits_p, p_boost_p, aspect_p, aspect_top1_plain_p = predict_aspect_boosted(\n","            new_prev_tokens,\n","            lambda_boost=lambda_boost,\n","            gamma=gamma,\n","            seed_bonus=seed_bonus,\n","            dampen_price_if_no_seed=dampen_price_if_no_seed,\n","            price_delta=price_delta,\n","            prefer_seed_for_top1=prefer_seed_for_top1\n","        )\n","        prob_p = p_boost_p[aspect_p]\n","\n","        adjusted[-1] = {\n","            \"seg_text\": new_prev_text,\n","            \"tokens\": new_prev_tokens,\n","            \"p_boost\": p_boost_p,\n","            \"seed_hits\": hits_p,\n","            \"aspect_final\": aspect_p,\n","            \"aspect_prob_final\": prob_p,\n","        }\n","\n","        # segmen sekarang = tail (kalau ada)\n","        if tail_text:\n","            new_toks = tokenize_from_val(tail_text)\n","            p_raw_c, hits_c, p_boost_c, aspect_c, aspect_top1_plain_c = predict_aspect_boosted(\n","                new_toks,\n","                lambda_boost=lambda_boost,\n","                gamma=gamma,\n","                seed_bonus=seed_bonus,\n","                dampen_price_if_no_seed=dampen_price_if_no_seed,\n","                price_delta=price_delta,\n","                prefer_seed_for_top1=prefer_seed_for_top1\n","            )\n","            prob_c = p_boost_c[aspect_c]\n","\n","            adjusted.append({\n","                \"seg_text\": tail_text,\n","                \"tokens\": new_toks,\n","                \"p_boost\": p_boost_c,\n","                \"seed_hits\": hits_c,\n","                \"aspect_final\": aspect_c,\n","                \"aspect_prob_final\": prob_c,\n","            })\n","\n","    # ===== 3) MERGE segmen berurutan dengan aspek sama =====\n","    merged = []\n","    for item in adjusted:\n","        if not merged:\n","            merged.append(item)\n","            continue\n","\n","        prev = merged[-1]\n","        if item[\"aspect_final\"] == prev[\"aspect_final\"]:\n","            combined_text = prev[\"seg_text\"].rstrip(\" ,\") + \" \" + item[\"seg_text\"].lstrip(\" ,\")\n","            combined_tokens = tokenize_from_val(combined_text)\n","            p_raw2, hits2, p_boost2, aspect2, aspect_top1_plain2 = predict_aspect_boosted(\n","                combined_tokens,\n","                lambda_boost=lambda_boost,\n","                gamma=gamma,\n","                seed_bonus=seed_bonus,\n","                dampen_price_if_no_seed=dampen_price_if_no_seed,\n","                price_delta=price_delta,\n","                prefer_seed_for_top1=prefer_seed_for_top1\n","            )\n","            prob2 = p_boost2[aspect2]\n","\n","            merged[-1] = {\n","                \"seg_text\": combined_text,\n","                \"tokens\": combined_tokens,\n","                \"p_boost\": p_boost2,\n","                \"seed_hits\": hits2,\n","                \"aspect_final\": aspect2,\n","                \"aspect_prob_final\": prob2,\n","            }\n","        else:\n","            merged.append(item)\n","\n","    # ===== 4) CETAK HASIL =====\n","    print(f'TEKS UTUH: \"{text}\"\\n')\n","\n","    print(\"SEGMENTASI TEKS:\")\n","    for i, r in enumerate(merged, start=1):\n","        print(f\"  Seg {i}: {r['seg_text']}\")\n","    print()\n","\n","    print(\"PROBABILITAS ASPEK FINAL PER SEGMEN:\")\n","    for i, r in enumerate(merged, start=1):\n","        print(f\"  Seg {i}: {r['aspect_final']} ({r['aspect_prob_final']:.4f})\")\n","\n","    print(\"\\nASPEK FINAL PER SEGMEN:\")\n","    for i, r in enumerate(merged, start=1):\n","        print(f\"  Seg {i}: {r['aspect_final']}\")\n","\n","    results = []\n","    for i, r in enumerate(merged, start=1):\n","        results.append({\n","            \"seg_index\": i,\n","            \"seg_text\": r[\"seg_text\"],\n","            \"p_boost\": r[\"p_boost\"],\n","            \"seed_hits\": r[\"seed_hits\"],\n","            \"aspect_final\": r[\"aspect_final\"],\n","            \"aspect_prob_final\": r[\"aspect_prob_final\"],\n","        })\n","    return results"],"metadata":{"id":"o9qYEtt8Kx7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==== CONTOH PANGGILAN TEST ====\n","hasil = test_segmented_text(\n","    \"​​Kemasannya tube kecil dan tutupnya rapet jadi aman kalo masuk tas, aromanya wangi segar kayak citrus tapi ga berlebihan, teksturnya creamy tapi pas dioles berasa ringan dan ga lengket, harganya sih di tengah-tengah ga murah banget tapi ga mahal juga, dan efeknya di aku bikin kulit lebih cerah merata dan berasa lebih halus. \"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoTPS05zMAP9","executionInfo":{"status":"ok","timestamp":1764173093510,"user_tz":-420,"elapsed":100,"user":{"displayName":"21-093_Adz Dzikry Pradana Putra","userId":"01565016450624471857"}},"outputId":"d3bd2793-e7d1-4a7f-e94f-8ac8972af3c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TEKS UTUH: \"​​Kemasannya tube kecil dan tutupnya rapet jadi aman kalo masuk tas, aromanya wangi segar kayak citrus tapi ga berlebihan, teksturnya creamy tapi pas dioles berasa ringan dan ga lengket, harganya sih di tengah-tengah ga murah banget tapi ga mahal juga, dan efeknya di aku bikin kulit lebih cerah merata dan berasa lebih halus. \"\n","\n","SEGMENTASI TEKS:\n","  Seg 1: ​​Kemasannya tube kecil dan tutupnya rapet jadi aman kalo masuk tas\n","  Seg 2: aromanya wangi segar kayak citrus tapi ga berlebihan\n","  Seg 3: teksturnya creamy tapi pas dioles berasa ringan dan ga lengket\n","  Seg 4: harganya sih di tengah-tengah ga murah banget tapi ga mahal juga, dan\n","  Seg 5: efeknya di aku bikin kulit lebih cerah merata dan berasa lebih halus.\n","\n","PROBABILITAS ASPEK FINAL PER SEGMEN:\n","  Seg 1: Kemasan (0.8961)\n","  Seg 2: Aroma (0.9613)\n","  Seg 3: Tekstur (0.7995)\n","  Seg 4: Harga (0.9584)\n","  Seg 5: Efek (0.9832)\n","\n","ASPEK FINAL PER SEGMEN:\n","  Seg 1: Kemasan\n","  Seg 2: Aroma\n","  Seg 3: Tekstur\n","  Seg 4: Harga\n","  Seg 5: Efek\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"W4eeLiA50Q7v"},"execution_count":null,"outputs":[]}]}